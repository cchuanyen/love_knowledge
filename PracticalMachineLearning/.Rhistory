rnorm(5)
dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
x <- rnorm(10)
x
x <- rnorm(10, 20, 2)
x
summary(x)
set.seed(1)
rnorm(5)
rnorm(5)
set.seed(1)
rnorm(5)
rpois(10, 1)
rpois(10, 2)
rpois(10, 20)
ppois(2, 2) ## Cumulative distribution
ppois(4, 2)
ppois(6, 2)
set.seed(20)
x <- rnorm(100, 0, 2)
e <- rnorm(100, 0, 2)
y <- 0.5 + 2 * x + e
summary(y)
plot(x, y)
set.seed(1)
x <- rnorm(100)
log.mu <- 0.5 + 0.3 * x
y <- rpois(100, exp(log.mu))
summary(y)
plot(x, y)
set.seed(1)
sample(1:10, 4)
sample(1:10, 4)
sample(letters, 5)
sample(1, 10) ## permutation
sample(1:10)
sample(1:10, replace = TRUE)
sample(1:10, replace = TRUE) Sample w/replacement
## Elapsed time > user time
hilbert <- function(n) {
i <- 1:n
1 / outer (i - l, i, "+")
}
x <- hilbert(1000)
system.time(svd(x))
1 / outer (i - 1, i, "+")
system.time({)
summaryRprof()
## lm(y ~ x)
sample.interval=10000
"list" "eval" "eval" "model.frame.default" "model.frame" "eval" "eval" "lm"
"lm.fit" "lm"
"by.total"
"by.self"
$by.total
"lm"
"lm.fit"
"model.frame.default"
"eval"
"na.omit"
"na.omit.data.frame"
"lapply"
$by.self
"as.character"
"model.frame.default"
"anyDuplicated.default"
$sample.interval
[1] 0.02
$sampling.time
set.seed(1)
rpois(5,2)
set.seed(10)
x <- rep(0:1, each = 5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
setwd("./data")
getwd()
if(!file.exists("data")){
dir.create("data")
}
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/%2Fdata%2Fss06hid.csv", method = "curl")
download.file(fileUrl, destfile = "./data/%2Fdata%2Fss06hid.csv")
## [1] "%2Fdata%2Fss06hid.csv"
dateDownloaded <- date()
dateDownloaded
load("~/.RData")
getwd()
idahoData <- read.csv("%2Fdata%2Fss06hid.csv")
read.csv
idahoData <- ("./%2Fdata%2Fss06hid.csv")
head(idahoData)
getwd()
if(!file.exists("data"))
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/%2Fdata%2Fss06hid.csv")
dateDownloaded <- date()
dateDownloaded
idahoData <- read.table("./data/%2Fdata%2Fss06hid.csv", sep = ",", header = TRUE)
head(idahoData)
getwd()
list.files(".")
dateDownloaded <- date()
dateDownloaded
Idaho <- read.csv("Idaho.csv")
Idaho <- read.csv
head(Idaho)
download.file(u)rl=fileUrl,destfile="getdata%2Fdata%2Fss06hid.csv"
download.file(url=fileUrl,destfile="getdata%2Fdata%2Fss06hid.csv")
list.files(".")
dateDownloaded <- date()
dateDownloade
dateDownloaded
Idaho <- read.csv("getdata%2Fdata%2Fss06hid.csv")
head(Idaho)
length(Idaho$VAL[!is.na(Idaho$VAL) & Idaho$VAL==24])
if(!file.exists("data")){dir.create("data")}
fileUrl <- https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru.rows.xlsx?accessType=DOWNLOAD""
fileUrl <- "https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru.rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx", method="curl")
download.file(fileUrl,destfile="./data/cameras.xlsx")
dateDownloaded <- date()
dateDownloaded
fileUrl <- "https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru.rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx")
library(xlsx)
if(!file.exists("data")){dir.create("data")}
if(!file.exists("data")){dir.create("data")}
fileUrl <- "https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru.rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx")
xpathSApply(rootNode,"//name", xmlValue)
fileUrl <- "http://espn.go.com/nfl/team/_?name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
DT[,list(mean(x),sum(z))]
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names)
exit
quit
Idaho <- read.csv("getdata-data-ss06hid.csv")
getwd()
Idaho <- read.csv("getdata-data-ss06hid.csv")
download.file(fileUrl,destfile="./data/cameras.xlsx", method="curl")
getwd
getwd()
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
library(httr); html2-GET(url)
content2=content(html2,as="text")
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)
xpathSApply(html, "//title", xmlValue)
pgl = GET("http://httpbin.org/basic-auth/user/passwd")
pgl
Response[http://httpbin.org/basic-auth/user/passwd]
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
setwd(C:\Users\COMPAQ\datasciencecoursera\Module 4 Course Project 1)
library(datasets)
library(lattice)
## Convert 'Month' to a factor variable
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
x <- x + f - f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c("Group 1", "Group 2"))
xyplot(y ~ x | f, layout = c(2, 1)) ## Plot with 2 panels
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(datasets)
data(airquality)
qplot(Wind, Ozone, Data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factorMonth)
library(ggplot2)
install.packages(ggplot2)
install.packages(ggplot)
g <- ggplot(movies, aes(votes, rating))
install.packages(ggplot2)
library(ggplot2)
install.packages(ggplot2)
gplot(votes, rating, data = movies)
names(trainSpam)
library(kernlab)
install.packages(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
set.seed(3435)
trainIndicator = rbinom(4601, size = 1, prob = 0.5)
table(trainIndicator)
trainSpam = spam[trainIndicator == 1, ]
testSpam = spam[trainIndicator == 0, ]
names(trainSpam)
View(trainSpam)
plot(trainspam$capitalAve ~ trainSpam$type)
table(trainSpam$type)
plot(trainspam$capitalAve ~ trainSpam$type)
plot(trainSpam$capitalAve ~ trainSpam$type)
plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)
View(testSpam)
View(trainSpam)
plot(log10(trainSpam[, 1:4] + 1))
hCluster = hclust(distr(t(trainSpam[, 1:57])))
hCluster = hclust(dist(t(trainSpam[, 1:57])))
My first R Markdown document
===========================
```{r}
```{r} My First R Markdown document```
### loading the car dataset
In this section, I shall explain how to load the build-in dataset named **mtcars**
library(knitr)
knit2html("document.Rmd")
browseURL("document.html")
```{r}
library(datasets)
library(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2")
str(mtcars)```
```{r} ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(colour=factor(cyl), size = qsec))```
My first R Markdown document
============================
### loading the car dataset
In this section, I shall explain how to load the build-in dataset named **mtcars**.
library(knitr)
knit2html("document.Rmd")
browseURL("document.html")
install.packages("Rmd")
```{r}
library(datasets)
library(ggplot2)
str(
```{r}
library(datasets)
library(ggplot2)
str(mtcars)```
```{r}
ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(colour=factor(cyl), size = qsec))
```
```{r} library(datasets) library(ggplot2) str(mtcars)```
```{r}
ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(colour=factor(cyl), size = qsec))
```
```{r echo=FALSE}   HIST(IRIS[[2]])```
install.packages("R.utils")
install.packages("gridExtra")
install.packages("grid")
install.packages("ggplot2")
install.packages("plyr")
x = 2 : 5
p = (1 : 4) / 10
p
sum(p)
rbind(x, p)
sum(x ^ 2 * p) - sum(x * p)^2
x <- 1:4
p <- x/sum(x)
temp <- rbind(x, p)
rownames(temp) <- c("X", "Prob")
temp
sum(x ^ 1 * p) - sum(x * p)^1
sum(x ^ 2 * p) - sum(x * p)^2
sum(x ^ 2) - sum(x)^2
sum(x * p)/10
(.52 * .75) / (.55 * .75 + .7 * .3)
(.52 * .75) / (.55 * .75 + .07 * .03)
(.52 * .75) / (.55 * .75 + .29 * .03)
n <- 9
1100 + c(-1, 1) * qt(.975, n - 1) * 30 / sqrt(n)
n <- 9
(sd <- 2 * sqrt(n) / qt(.975, n - 1))
σ_p <- sqrt(((n_x - 1) * var_x + (n_y - 1) * var_y)/(n_x + n_y - 2))
confidenceInterval <- μ_y - μ_x + c(-1, 1) * qt(quantile, df=n_y+n_x-2) * σ_p * (1 / n_x + 1 / n_y)^.5
round(confidenceInterval,2)
σ_p <- sqrt(((n_x - 1) * σ_x^2 + (n_y - 1) * σ_y^2)/(n_x + n_y - 2))
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
swirl()
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
exit
library(swirl)
ls
rm(list=ls())
swirl()
3
info()
skip()
deck
52
4/52
0
3/52
12/52
2/13
2/12
info()
2/51
6.4
6.4
info()
skip()
.64
mypdf
function(1.6){1.6/2}
(1.6){1.6/2}
skip()
0.8
x^2=4*.5=2
skip()
swirl()
.000997
1.997001
0.014985
0.06238268
1/6
1
21/6
1
expect_dice
dice_high
function(pmf){ mu <- 0; for (i in 1:6) mu <- mu + i*pmf[i]; mu}
expect_dice(dice_high)
expect_dice(dice_low)
E((X_hi + X_lo)/2) or .5 *( E(X_hi)+E(X_lo) )
E((X_hi + X_lo)/2) .5 *( E(X_hi)+E(X_lo) )
E((X_hi + X_lo)/2)
expect_dice(dice_low)
.5*(edh+edl)
f(t/2)
integrate (the lower bound) and (the upper bound)
t^2/2
2.0/2
integrate(myfunc,0,2)
x(1,5)
P(1,5)
(x=1, 5)
x <- 1,5
info()
info()
skip()
spop
mean(spop)
allsam
mean(allsam)
apply(allsam,1,mean)
smeans
mean(smeans)
subject <- c(1,2,3,4,5)
baseline <- c(140,138,150,148,135)
week2 <- c(132, 135, 151, 146, 130)
examinations <- data.frame(subject, baseline, week2)
examinations
test <- t.test(x=examinations$baseline, y = examinations$week2, alt = "two.sided", paired = TRUE)
pval <- test$p.value
round(pval,3)
n <- 9
μ <- 1100σ <- 30
quantile = 0.975 # is 95% with 2.5% on both sides of the range
confidenceInterval = μ + c(-1, 1) * qt(quantile, df=n-1) * σ / sqrt(n)
confidenceInterval
μ <- 1100
σ <- 30
quantile = 0.975 # is 95% with 2.5% on both sides of the range
confidenceInterval = μ + c(-1, 1) * qt(quantile, df=n-1) * σ / sqrt(n)
confidenceInterval
n <- 4
x <- 3
test <- binom.test(x=x, n=n, alt="greater")
round(test$p.value,2)
rate <- 1/100
errors <- 10
days <- 1787
test <- poisson.test(errors, T = days, r = rate, alt="less")
round(test$p.value,2)
n_y <- 9 # subjects treated
n_x <- 9 # subjects placebo
σ_y <- 1.5 # kg/m2 std.dev. treated
σ_x <- 1.8 # kg/m2 std.dev. placebo
μ_y <- -3 # kg/m2 average difference treated
μ_x <- 1 # kg/m2 average difference placebo
# calculate pooled standard deviation
σ_p <- (((n_x - 1) * σ_x^2 + (n_y - 1) * σ_y^2)/(n_x + n_y - 2))
pval <- pt((μ_y - μ_x) / (σ_p * (1 / n_x + 1 / n_y)^.5), df=n_y + n_x - 2)
pval
n <- 100 # subject
μ <- 0.01 # m^3 brain volume loss mean
σ <- 0.04 # m^3 train volume loss std. dev.
p <- 0.05 # sign level
pow <- power.t.test(n=n, delta=μ, sd=σ, sig.level=p, type="one.sample", alt="one.sided")$power
round(pow, 2)
μ <- 0.01 # m^3 brain volume loss mean
σ <- 0.04 # m^3 train volume loss std. dev.
p <- 0.05 # sign level
pow <- 0.9 # power
n <- power.t.test(power=pow, delta=μ, sd=σ , sig.level=p, type=one.sample", alt="one.sided")$n
ceiling(n/10)*10
n <- power.t.test(power=pow, delta=μ, sd=σ , sig.level=p, type="one.sample", alt="one.sided")$n
ceiling(n/10)*10
install.packages("UsingR")
library(mtcars)
data(mtcars)
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
weighted.mean(x, w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ 0 + x)$coeff
data(mtcars)
lm(mpg ~ wt, data=mtcars)
0.5 * 1 / 0.5
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x - mean(x)) / sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ x)$coef
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
coefTable <- coef(summary(fit))
(pval <- coefTable[2, 4])
n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e ^ 2) / (n - 2))
ssx <- sum((x - mean(x)) ^ 2)
seBeta1 <- sigma / sqrt(ssx)
tBeta1 <- beta1 / seBeta1
(pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE))
summary(fit)$sigma
(sigma <- sqrt(sum(e ^ 2) / (n - 2)))
data(mtcars)
y <- mtcars$mpg
x <- mtcars$wt
fit_car <- lm(y ~ x)
predict(fit_car, newdata = data.frame(x = mean(x)), interval = ("confidence"))
yhat <- fit_car$coef[1] + fit_car$coef[2] * mean(x)
yhat + c(-1, 1) * qt(.975, df = fit_car$df) * summary(fit_car)$sigma / sqrt(length(y))
predict(fit_car, newdata = data.frame(x = 3), interval = ("prediction"))
yhat <- fit_car$coef[1] + fit_car$coef[2] * 3
yhat + c(-1, 1) * qt(.975, df = fit_car$df) * summary(fit_car)$sigma * sqrt(1 + (1/length(y)) + ((3 - mean(x)) ^ 2 / sum((x - mean(x)) ^ 2)))
fit_car2 <- lm(y ~ I(x/2))
sumCoef2 <- coef(summary(fit_car2))
(sumCoef2[2,1] + c(-1, 1) * qt(.975, df = fit_car2$df) * sumCoef2[2, 2])
data(mtcars)
y <- mtcars$mpg
x <- mtcars$wt
fit_car <- lm(y ~ x)
sum(resid(fit_car)^2) / sum((y - mean(y)) ^ 2)
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)$coefficient
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
library(caret)
install.packages("caret")
library(caret)
install.packages("randomForest")
library(randomForest)
install.pacakages("rpart")
install.packages("rpart")
library(rpart)
train<-read.csv("pml-training.csv",na.strings=c("NA",""), strip.white = T)
test <-read.csv("pml-testing.csv", na.strings=c("NA",""), strip.white = T)
trainingset <- read.csv("C:\Users\COMPAQ\Documents\DATA SCIENTIST\Practical Machine Learning\pml-training.csv", na.strings=c("NA", "#DIV/0!", "")
trainingset <- read.csv("C:Users\COMPAQ\Documents\DATA SCIENTIST\Practical Machine Learning\pml-training.csv", na.strings=c("NA", "#DIV/0!", "")
trainingset <- read.csv("C:\Users\COMPAQ\Documents\DATA SCIENTIST\Practical Machine Learning\pml-training.csv", na.strings=c("NA", "#DIV/0!", "")
setwd("~/DATA SCIENTIST/Practical Machine Learning")
trainingset <- read.csv("pml-training")
trainingset <- read.csv("pml-training.csv")
testingset <- read.csv("pml-testing.csv")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
